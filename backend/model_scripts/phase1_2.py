# -*- coding: utf-8 -*-
"""phase1.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QxDVBD2RD7Tx792cuFIejdb_I3yFCWci
"""

import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define the folder containing multiple stock CSVs
folder_path = '/content/drive/MyDrive/stocks/'

# List all stock CSV files
stock_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

# Initialize an empty list to hold data from all stocks
aligned_stock_data = pd.DataFrame()
# Loop through and read each stock CSV
file_count = 1
for stock_file in stock_files:
    # print(f"Processing file {file_count}/{len(stock_files)}: {stock_file}")
    file_count += 1
    filepath = os.path.join(folder_path, stock_file)
    df = pd.read_csv(filepath)

    # Keep only the relevant columns and rename the Close column to include the stock name
    #
    stock_name = stock_file.split('.')[0]
    stock_data = df[['Date', 'Prev Close','Open','High','Low','Last','Close','VWAP','Volume','Turnover','Trades','Deliverable Volume']].rename(columns={
        'Prev Close' : stock_name + '_Prev Close',
        'Open' : stock_name + '_Open',
        'High' : stock_name + '_High',
        'Low' : stock_name + '_Low',
        'Last' : stock_name + '_Last',
        'Close': stock_name + '_Close',
        'VWAP' : stock_name + '_VWAP',
        'Volume' : stock_name + '_Volume',
        'Turnover' : stock_name + '_Turnover',
        'Trades' : stock_name + '_Trades',
        'Deliverable Volume' : stock_name + '_Deliverable Volume'
        })
    stock_data['Date'] = pd.to_datetime(stock_data['Date'])
    # Merge each stock based on Date to align the data
    if aligned_stock_data.empty:
        aligned_stock_data = stock_data  # If first stock, initialize the DataFrame
    else:
        aligned_stock_data = pd.merge(aligned_stock_data, stock_data, on='Date', how='outer')  # Merge based on Date
    # print(aligned_stock_data.shape)

# Drop rows with any missing values
aligned_stock_data.dropna(inplace=True)

# Initialize a list to hold the new columns
new_columns = []

# Generate shifted columns and store them in the list
for stock in stock_files:
    stock_name = stock.split('.')[0]
    new_columns.append(aligned_stock_data[f'{stock_name}_Close'].shift(-1).rename(f'{stock_name}_Close1'))
    new_columns.append(aligned_stock_data[f'{stock_name}_Close'].shift(-7).rename(f'{stock_name}_Close7'))
    new_columns.append(aligned_stock_data[f'{stock_name}_Close'].shift(-14).rename(f'{stock_name}_Close14'))
    new_columns.append(aligned_stock_data[f'{stock_name}_Close'].shift(-21).rename(f'{stock_name}_Close21'))
    new_columns.append(aligned_stock_data[f'{stock_name}_Close'].shift(-30).rename(f'{stock_name}_Close30'))

# Concatenate new columns to the DataFrame
aligned_stock_data = pd.concat([aligned_stock_data] + new_columns, axis=1)

# Drop rows with NaN values introduced by shifting (as these have no labels)
aligned_stock_data.dropna(inplace=True)

aligned_stock_data.shape

# Separate features (X) and targets (Y) for all stocks
X = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Date' not in col and 'Close1' not in col and 'Close7' not in col and 'Close14' not in col and 'Close21' not in col and 'Close30' not in col]]
# X = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close' in col
Y = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close1' in col or 'Close7' in col or 'Close14' in col or 'Close21' in col or 'Close30' in col]]
Y1 = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close14' not in col and 'Close1' in col]]
Y7 = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close7' in col]]
Y14 = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close14' in col]]
Y21 = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close21' in col]]
Y30 = aligned_stock_data[[col for col in aligned_stock_data.columns if 'Close30' in col]]


import numpy as np

# Reshape X to be 3D: (samples, timesteps, features)
timesteps = 30  # Assuming you want to use the last 30 days to predict the next day

# Number of features (49 stocks * 11 features per stock = 539 features)
num_features = 539

# Total number of rows in the dataset
n_samples = X.shape[0]


def create_sequences(xdata, ydata, time_steps):
    x, y = [], []
    for i in range(len(xdata) - time_steps):
        x.append(xdata[i:i + time_steps])
        y.append(ydata.iloc[i + time_steps].values)

    return np.array(x), np.array(y)

X_scaled = StandardScaler().fit_transform(X)

# Adjust the data to fit into complete timesteps
X_adjusted, Y1_adjusted = create_sequences(X_scaled, Y1, timesteps)

xtrain, xtest, ytrain1, ytest1 = train_test_split(X_adjusted, Y1_adjusted[:,0], test_size=0.2, shuffle=False)

from keras.models import Sequential
from keras.layers import LSTM, Dense, GRU, Dropout, BatchNormalization, SimpleRNN
from keras.optimizers import Adam
import numpy as np
from keras.callbacks import EarlyStopping
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from tensorflow.keras.callbacks import ReduceLROnPlateau

early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)


input_shape = (xtrain.shape[1], xtrain.shape[2])

def evaluate_deep_learning_model(model, xtest, ytest, name):
    # Predict values
    ypred = model.predict(xtest)

    mse = mean_squared_error(ytest, ypred)
    mape = mean_absolute_percentage_error(ytest, ypred) * 100
    r2 = r2_score(ytest, ypred)

    print(f"{name} - MSE: {mse}, MAPE: {mape}, R2: {r2}")


def build_lstm_model(input_shape):
    model = Sequential()

    model.add(LSTM(1024, activation='tanh', return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(LSTM(1024, activation='tanh', return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(LSTM(512, activation='tanh', return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(LSTM(256, activation='tanh', return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(LSTM(256, activation='tanh', return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(LSTM(128, activation='tanh', return_sequences=False))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(Dense(64, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))

    model.add(Dense(1, activation='linear'))

    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='mse')

    return model


# Build and train LSTM model
lstm_model = build_lstm_model(input_shape)

# Example of fitting the model with callbacks
history_lstm = lstm_model.fit(xtrain, ytrain1, epochs=600, batch_size=128, validation_data=(xtest, ytest1), verbose=1)

pred_lstm = evaluate_deep_learning_model(lstm_model, xtest, ytest1, name="LSTM")

def build_gru_model(input_shape):
    model = Sequential()

    # 1st GRU layer with 128 units
    model.add(GRU(128, activation='tanh', return_sequences=True, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    # 2nd GRU layer with 64 units
    model.add(GRU(128, activation='tanh', return_sequences=True))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(GRU(64, activation='tanh', return_sequences=False))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    # Dense layer for prediction (multi-output)
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))

    model.add(Dense(5))  # Final layer with 5 outputs

    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='mse')
    return model

gru_model = build_gru_model(input_shape)
history_gru = gru_model.fit(xtrain_nn, ytrain, epochs=200, batch_size=72, validation_data=(xtest_nn, ytest), verbose=2)
pred_gru = evaluate_deep_learning_model(gru_model, xtest_nn, ytest, name="GRU")