{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "companies = [\n",
    "    'MM',\n",
    "    'NESTLEIND',\n",
    "    'NTPC',\n",
    "    'ONGC',\n",
    "    'POWERGRID',\n",
    "    'RELIANCE',\n",
    "    'SBIN',\n",
    "    'SHREECEM',\n",
    "    'SUNPHARMA',\n",
    "    'TATAMOTORS',\n",
    "    'TATASTEEL',\n",
    "    'TCS',\n",
    "    'TECHM',\n",
    "    'TITAN',\n",
    "    'ULTRACEMCO',\n",
    "    'UPL',\n",
    "    'VEDL',\n",
    "    'WIPRO',\n",
    "    'ZEEL'\n",
    "]\n",
    "\n",
    "print(len(companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MM model...\n",
      "MM model saved\n",
      "Training NESTLEIND model...\n",
      "NESTLEIND model saved\n",
      "Training NTPC model...\n",
      "NTPC model saved\n",
      "Training ONGC model...\n",
      "ONGC model saved\n",
      "Training POWERGRID model...\n",
      "POWERGRID model saved\n",
      "Training RELIANCE model...\n",
      "RELIANCE model saved\n",
      "Training SBIN model...\n",
      "SBIN model saved\n",
      "Training SHREECEM model...\n",
      "SHREECEM model saved\n",
      "Training SUNPHARMA model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mrmuf\\miniconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for company in companies:\n",
    "    K.clear_session()\n",
    "    df = pd.read_csv(f'stocks_new/{company}.csv')\n",
    "\n",
    "    prices = df['Close'].values.tolist()\n",
    "\n",
    "    def clean_stock_prices(stock_prices):\n",
    "        cleaned_prices = []\n",
    "        for price in stock_prices:\n",
    "\n",
    "            if isinstance(price, str):\n",
    "                price = price.replace(',', '').strip()  # Remove commas\n",
    "            try:\n",
    "                price = float(price)\n",
    "                if not np.isnan(price):  # Check for NaN values\n",
    "                    cleaned_prices.append(price)\n",
    "            except ValueError:\n",
    "                pass  # Skip invalid values\n",
    "\n",
    "        return np.array(cleaned_prices).reshape(-1,1)\n",
    "\n",
    "    prices = clean_stock_prices(prices)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_prices = scaler.fit_transform(prices)\n",
    "\n",
    "    def create_sequences(data, sequence_length):\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        for i in range(len(data) - (sequence_length+30)):\n",
    "            sequences.append(data[i:i+sequence_length])\n",
    "            labels.append(data[i+sequence_length+30])\n",
    "\n",
    "        return np.array(sequences), np.array(labels)\n",
    "\n",
    "    sequence_length = 60\n",
    "    X, y = create_sequences(scaled_prices, sequence_length)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building the LSTM model\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    print(f'Training {company} model...')\n",
    "    history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    model.save(f'models\\{company}_model.h5')\n",
    "    print(f'{company} model saved')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWZrbT1pQxOo",
    "outputId": "136c9448-ed94-4145-b2fa-72a718ddd4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print(keras.backend.backend())\n",
    "# output should be \"tensorflow\"\n",
    "\n",
    "df = pd.read_csv('stocks_new/BPCL.csv')\n",
    "\n",
    "prices = df['Close'].values.tolist()\n",
    "\n",
    "def clean_stock_prices(stock_prices):\n",
    "    cleaned_prices = []\n",
    "    for price in stock_prices:\n",
    "\n",
    "        if isinstance(price, str):\n",
    "            price = price.replace(',', '').strip()  # Remove commas\n",
    "        try:\n",
    "            price = float(price)\n",
    "            if not np.isnan(price):  # Check for NaN values\n",
    "                cleaned_prices.append(price)\n",
    "        except ValueError:\n",
    "            pass  # Skip invalid values\n",
    "\n",
    "    return np.array(cleaned_prices).reshape(-1,1)\n",
    "\n",
    "prices = clean_stock_prices(prices)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_prices = scaler.fit_transform(prices)\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "  sequences = []\n",
    "  labels = []\n",
    "  for i in range(len(data) - (sequence_length+30)):\n",
    "    sequences.append(data[i:i+sequence_length])\n",
    "    labels.append(data[i+sequence_length+30])\n",
    "\n",
    "  return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 60\n",
    "X, y = create_sequences(scaled_prices, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDyNJEF09pt0",
    "outputId": "080862d8-48ad-4103-c7fa-c98b423ae822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'prices': 0\n",
      "[9.79]\n"
     ]
    }
   ],
   "source": [
    "nan_count = np.isnan(prices).sum()\n",
    "print(f\"Number of NaN values in 'prices': {nan_count}\")\n",
    "print(prices[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mlvLO2bSZEZ9",
    "outputId": "7b812d8d-eccb-43aa-daa0-723d325d7fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "162/162 [==============================] - 8s 26ms/step - loss: 0.0058 - val_loss: 0.0028\n",
      "Epoch 2/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 3/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 4/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 5/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 6/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 7/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 8/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 9/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 10/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 11/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 12/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 13/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 14/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 15/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 16/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 17/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 18/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 19/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 20/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 21/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 22/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 23/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 24/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 25/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 26/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 27/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 28/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 29/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 30/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 31/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 32/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 33/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 34/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 35/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 36/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 37/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 38/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 39/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 40/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 41/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 42/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 43/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 44/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 45/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 46/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 47/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 48/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 49/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 50/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 51/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 52/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 53/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 54/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 55/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 56/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 57/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 58/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 59/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 60/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 61/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 62/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 63/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 64/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 65/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 66/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 67/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 68/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 69/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 70/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 71/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 72/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 73/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 74/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 75/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 76/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 77/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 78/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 79/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 80/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 81/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 82/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 83/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 84/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0011 - val_loss: 9.8337e-04\n",
      "Epoch 85/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 86/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 87/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 88/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 89/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 90/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 9.9753e-04 - val_loss: 9.2940e-04\n",
      "Epoch 93/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 9.9476e-04 - val_loss: 9.9363e-04\n",
      "Epoch 94/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0010 - val_loss: 9.3830e-04\n",
      "Epoch 95/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 96/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0011 - val_loss: 9.2952e-04\n",
      "Epoch 97/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 98/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.6007e-04 - val_loss: 9.0960e-04\n",
      "Epoch 99/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.0205e-04 - val_loss: 8.6012e-04\n",
      "Epoch 100/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.2886e-04 - val_loss: 8.7800e-04\n",
      "Epoch 101/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 9.2106e-04 - val_loss: 8.4308e-04\n",
      "Epoch 102/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.0780e-04 - val_loss: 9.8439e-04\n",
      "Epoch 103/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 8.8753e-04 - val_loss: 9.3007e-04\n",
      "Epoch 104/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 8.4307e-04 - val_loss: 7.9173e-04\n",
      "Epoch 105/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 8.6706e-04 - val_loss: 8.7752e-04\n",
      "Epoch 106/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0010 - val_loss: 7.4358e-04\n",
      "Epoch 107/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 9.1163e-04 - val_loss: 0.0015\n",
      "Epoch 108/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 0.0010 - val_loss: 8.5892e-04\n",
      "Epoch 109/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 7.9329e-04 - val_loss: 7.4056e-04\n",
      "Epoch 110/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 8.1030e-04 - val_loss: 0.0010\n",
      "Epoch 111/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 112/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.6217e-04 - val_loss: 0.0011\n",
      "Epoch 113/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 9.0122e-04 - val_loss: 8.9523e-04\n",
      "Epoch 114/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 8.2617e-04 - val_loss: 7.6616e-04\n",
      "Epoch 115/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 8.1679e-04 - val_loss: 0.0011\n",
      "Epoch 116/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 7.6191e-04 - val_loss: 7.1310e-04\n",
      "Epoch 117/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 8.0553e-04 - val_loss: 9.7669e-04\n",
      "Epoch 118/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.7786e-04 - val_loss: 8.8470e-04\n",
      "Epoch 119/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.4718e-04 - val_loss: 5.2674e-04\n",
      "Epoch 120/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.6773e-04 - val_loss: 6.4555e-04\n",
      "Epoch 121/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.3456e-04 - val_loss: 7.7859e-04\n",
      "Epoch 122/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.2446e-04 - val_loss: 6.1176e-04\n",
      "Epoch 123/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.1515e-04 - val_loss: 5.6357e-04\n",
      "Epoch 124/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.6175e-04 - val_loss: 3.9270e-04\n",
      "Epoch 125/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.0648e-04 - val_loss: 5.3225e-04\n",
      "Epoch 126/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 7.6755e-04 - val_loss: 6.5390e-04\n",
      "Epoch 127/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.8258e-04 - val_loss: 5.0829e-04\n",
      "Epoch 128/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.9780e-04 - val_loss: 4.8456e-04\n",
      "Epoch 129/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 7.7594e-04 - val_loss: 4.6675e-04\n",
      "Epoch 130/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 6.7996e-04 - val_loss: 5.3873e-04\n",
      "Epoch 131/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.7558e-04 - val_loss: 3.8829e-04\n",
      "Epoch 132/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.5721e-04 - val_loss: 4.3093e-04\n",
      "Epoch 133/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.1395e-04 - val_loss: 3.6860e-04\n",
      "Epoch 134/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.0927e-04 - val_loss: 4.6701e-04\n",
      "Epoch 135/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.6269e-04 - val_loss: 4.5203e-04\n",
      "Epoch 136/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.9895e-04 - val_loss: 5.3121e-04\n",
      "Epoch 137/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.1445e-04 - val_loss: 4.4305e-04\n",
      "Epoch 138/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.9837e-04 - val_loss: 4.2673e-04\n",
      "Epoch 139/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.6421e-04 - val_loss: 5.0366e-04\n",
      "Epoch 140/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 5.0248e-04 - val_loss: 3.8116e-04\n",
      "Epoch 141/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.7392e-04 - val_loss: 3.6560e-04\n",
      "Epoch 142/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.9309e-04 - val_loss: 5.2502e-04\n",
      "Epoch 143/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.7983e-04 - val_loss: 3.6043e-04\n",
      "Epoch 144/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.0917e-04 - val_loss: 3.5448e-04\n",
      "Epoch 145/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 5.1702e-04 - val_loss: 4.7891e-04\n",
      "Epoch 146/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.8633e-04 - val_loss: 3.9113e-04\n",
      "Epoch 147/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 5.7979e-04 - val_loss: 4.8880e-04\n",
      "Epoch 148/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.9008e-04 - val_loss: 3.9789e-04\n",
      "Epoch 149/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.3094e-04 - val_loss: 3.7324e-04\n",
      "Epoch 150/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.7496e-04 - val_loss: 3.1614e-04\n",
      "Epoch 151/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.6601e-04 - val_loss: 3.2841e-04\n",
      "Epoch 152/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.8294e-04 - val_loss: 4.1616e-04\n",
      "Epoch 153/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.8960e-04 - val_loss: 3.4226e-04\n",
      "Epoch 154/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.7625e-04 - val_loss: 3.1417e-04\n",
      "Epoch 155/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.3788e-04 - val_loss: 3.0900e-04\n",
      "Epoch 156/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.2028e-04 - val_loss: 3.2049e-04\n",
      "Epoch 157/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.5289e-04 - val_loss: 3.6648e-04\n",
      "Epoch 158/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.3013e-04 - val_loss: 2.8795e-04\n",
      "Epoch 159/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.1951e-04 - val_loss: 3.7489e-04\n",
      "Epoch 160/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.4788e-04 - val_loss: 4.0395e-04\n",
      "Epoch 161/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 5.6750e-04 - val_loss: 5.3953e-04\n",
      "Epoch 162/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.7671e-04 - val_loss: 3.0346e-04\n",
      "Epoch 163/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.4510e-04 - val_loss: 2.5052e-04\n",
      "Epoch 164/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.1270e-04 - val_loss: 2.5643e-04\n",
      "Epoch 165/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.0877e-04 - val_loss: 2.7884e-04\n",
      "Epoch 166/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 6.2227e-04 - val_loss: 5.8653e-04\n",
      "Epoch 167/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 6.2799e-04 - val_loss: 4.0979e-04\n",
      "Epoch 168/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.3877e-04 - val_loss: 3.0502e-04\n",
      "Epoch 169/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.1591e-04 - val_loss: 2.7467e-04\n",
      "Epoch 170/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.0007e-04 - val_loss: 2.8103e-04\n",
      "Epoch 171/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.1294e-04 - val_loss: 2.7931e-04\n",
      "Epoch 172/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.9112e-04 - val_loss: 2.3870e-04\n",
      "Epoch 173/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.5198e-04 - val_loss: 3.0068e-04\n",
      "Epoch 174/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.9557e-04 - val_loss: 2.6506e-04\n",
      "Epoch 175/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.9313e-04 - val_loss: 3.2074e-04\n",
      "Epoch 176/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.5077e-04 - val_loss: 3.5138e-04\n",
      "Epoch 177/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.1698e-04 - val_loss: 2.8532e-04\n",
      "Epoch 178/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.8453e-04 - val_loss: 3.9689e-04\n",
      "Epoch 179/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 6.7009e-04 - val_loss: 3.0910e-04\n",
      "Epoch 180/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.4128e-04 - val_loss: 3.6733e-04\n",
      "Epoch 181/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.8802e-04 - val_loss: 3.5415e-04\n",
      "Epoch 182/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7704e-04 - val_loss: 2.5253e-04\n",
      "Epoch 183/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.0910e-04 - val_loss: 2.7225e-04\n",
      "Epoch 184/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.4013e-04 - val_loss: 3.1199e-04\n",
      "Epoch 185/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.8433e-04 - val_loss: 2.8639e-04\n",
      "Epoch 186/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.3695e-04 - val_loss: 3.3165e-04\n",
      "Epoch 187/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.8039e-04 - val_loss: 2.6685e-04\n",
      "Epoch 188/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 4.0453e-04 - val_loss: 2.7018e-04\n",
      "Epoch 189/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.2595e-04 - val_loss: 2.9406e-04\n",
      "Epoch 190/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.3539e-04 - val_loss: 3.1076e-04\n",
      "Epoch 191/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.7723e-04 - val_loss: 2.3367e-04\n",
      "Epoch 192/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.8622e-04 - val_loss: 3.1486e-04\n",
      "Epoch 193/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.0913e-04 - val_loss: 3.7471e-04\n",
      "Epoch 194/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.1739e-04 - val_loss: 2.9154e-04\n",
      "Epoch 195/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.7217e-04 - val_loss: 2.5410e-04\n",
      "Epoch 196/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.8450e-04 - val_loss: 2.8491e-04\n",
      "Epoch 197/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.1268e-04 - val_loss: 3.2784e-04\n",
      "Epoch 198/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.2705e-04 - val_loss: 2.1898e-04\n",
      "Epoch 199/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.9991e-04 - val_loss: 3.2681e-04\n",
      "Epoch 200/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.0184e-04 - val_loss: 2.1982e-04\n",
      "Epoch 201/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.7593e-04 - val_loss: 2.5259e-04\n",
      "Epoch 202/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.8191e-04 - val_loss: 2.9037e-04\n",
      "Epoch 203/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.6332e-04 - val_loss: 2.5687e-04\n",
      "Epoch 204/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6470e-04 - val_loss: 2.2648e-04\n",
      "Epoch 205/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.8646e-04 - val_loss: 2.1886e-04\n",
      "Epoch 206/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6346e-04 - val_loss: 3.7116e-04\n",
      "Epoch 207/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7184e-04 - val_loss: 2.0383e-04\n",
      "Epoch 208/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.6332e-04 - val_loss: 5.0561e-04\n",
      "Epoch 209/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.8484e-04 - val_loss: 2.2991e-04\n",
      "Epoch 210/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4281e-04 - val_loss: 2.3740e-04\n",
      "Epoch 211/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6706e-04 - val_loss: 2.8806e-04\n",
      "Epoch 212/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.9761e-04 - val_loss: 3.1168e-04\n",
      "Epoch 213/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.5798e-04 - val_loss: 2.2482e-04\n",
      "Epoch 214/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.6579e-04 - val_loss: 2.6604e-04\n",
      "Epoch 215/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.6617e-04 - val_loss: 2.0959e-04\n",
      "Epoch 216/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.6319e-04 - val_loss: 2.3767e-04\n",
      "Epoch 217/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.5550e-04 - val_loss: 2.3341e-04\n",
      "Epoch 218/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.4203e-04 - val_loss: 2.2957e-04\n",
      "Epoch 219/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.5007e-04 - val_loss: 2.1777e-04\n",
      "Epoch 220/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.7051e-04 - val_loss: 2.3582e-04\n",
      "Epoch 221/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.5422e-04 - val_loss: 2.2611e-04\n",
      "Epoch 222/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.9243e-04 - val_loss: 4.0296e-04\n",
      "Epoch 223/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 5.8684e-04 - val_loss: 3.9480e-04\n",
      "Epoch 224/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7590e-04 - val_loss: 2.9199e-04\n",
      "Epoch 225/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.5054e-04 - val_loss: 2.1529e-04\n",
      "Epoch 226/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.5743e-04 - val_loss: 2.5742e-04\n",
      "Epoch 227/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4432e-04 - val_loss: 2.0040e-04\n",
      "Epoch 228/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6277e-04 - val_loss: 2.5490e-04\n",
      "Epoch 229/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6059e-04 - val_loss: 2.7472e-04\n",
      "Epoch 230/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.5789e-04 - val_loss: 2.8178e-04\n",
      "Epoch 231/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.9451e-04 - val_loss: 2.1558e-04\n",
      "Epoch 232/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7315e-04 - val_loss: 2.4386e-04\n",
      "Epoch 233/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7516e-04 - val_loss: 2.2650e-04\n",
      "Epoch 234/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.4796e-04 - val_loss: 2.2586e-04\n",
      "Epoch 235/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4922e-04 - val_loss: 2.0143e-04\n",
      "Epoch 236/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4423e-04 - val_loss: 2.5822e-04\n",
      "Epoch 237/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4306e-04 - val_loss: 1.9069e-04\n",
      "Epoch 238/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6013e-04 - val_loss: 3.0167e-04\n",
      "Epoch 239/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.6340e-04 - val_loss: 2.2641e-04\n",
      "Epoch 240/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.3661e-04 - val_loss: 2.2617e-04\n",
      "Epoch 241/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.6969e-04 - val_loss: 6.2585e-04\n",
      "Epoch 242/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.2958e-04 - val_loss: 3.3733e-04\n",
      "Epoch 243/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.2447e-04 - val_loss: 5.3446e-04\n",
      "Epoch 244/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 4.9654e-04 - val_loss: 4.4034e-04\n",
      "Epoch 245/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.4875e-04 - val_loss: 2.5116e-04\n",
      "Epoch 246/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.3241e-04 - val_loss: 2.1181e-04\n",
      "Epoch 247/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.3668e-04 - val_loss: 1.8746e-04\n",
      "Epoch 248/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.3001e-04 - val_loss: 2.1010e-04\n",
      "Epoch 249/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0000e-04 - val_loss: 2.0185e-04\n",
      "Epoch 250/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.2470e-04 - val_loss: 2.6054e-04\n",
      "Epoch 251/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.7270e-04 - val_loss: 1.9811e-04\n",
      "Epoch 252/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.4348e-04 - val_loss: 2.1265e-04\n",
      "Epoch 253/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.2161e-04 - val_loss: 2.1800e-04\n",
      "Epoch 254/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.4199e-04 - val_loss: 2.1943e-04\n",
      "Epoch 255/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.2207e-04 - val_loss: 1.8835e-04\n",
      "Epoch 256/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.1548e-04 - val_loss: 2.4460e-04\n",
      "Epoch 257/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.6944e-04 - val_loss: 2.1317e-04\n",
      "Epoch 258/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.6672e-04 - val_loss: 2.8080e-04\n",
      "Epoch 259/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.7618e-04 - val_loss: 1.9277e-04\n",
      "Epoch 260/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.3241e-04 - val_loss: 2.1631e-04\n",
      "Epoch 261/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2618e-04 - val_loss: 1.7913e-04\n",
      "Epoch 262/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0873e-04 - val_loss: 2.3400e-04\n",
      "Epoch 263/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.3547e-04 - val_loss: 1.9300e-04\n",
      "Epoch 264/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2449e-04 - val_loss: 1.7405e-04\n",
      "Epoch 265/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 3.2325e-04 - val_loss: 1.8787e-04\n",
      "Epoch 266/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2887e-04 - val_loss: 1.9707e-04\n",
      "Epoch 267/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2138e-04 - val_loss: 1.9371e-04\n",
      "Epoch 268/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.4467e-04 - val_loss: 1.9780e-04\n",
      "Epoch 269/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1353e-04 - val_loss: 2.1244e-04\n",
      "Epoch 270/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.2493e-04 - val_loss: 2.0848e-04\n",
      "Epoch 271/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.4520e-04 - val_loss: 2.2139e-04\n",
      "Epoch 272/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0658e-04 - val_loss: 2.2130e-04\n",
      "Epoch 273/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1895e-04 - val_loss: 1.7785e-04\n",
      "Epoch 274/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.3651e-04 - val_loss: 2.8873e-04\n",
      "Epoch 275/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.2896e-04 - val_loss: 2.1740e-04\n",
      "Epoch 276/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1733e-04 - val_loss: 2.0467e-04\n",
      "Epoch 277/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.3974e-04 - val_loss: 2.0213e-04\n",
      "Epoch 278/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.2026e-04 - val_loss: 2.0570e-04\n",
      "Epoch 279/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0768e-04 - val_loss: 3.7661e-04\n",
      "Epoch 280/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1586e-04 - val_loss: 1.9622e-04\n",
      "Epoch 281/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.4091e-04 - val_loss: 7.3539e-04\n",
      "Epoch 282/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.9891e-04 - val_loss: 2.1885e-04\n",
      "Epoch 283/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.3375e-04 - val_loss: 1.9590e-04\n",
      "Epoch 284/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2281e-04 - val_loss: 2.2773e-04\n",
      "Epoch 285/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2819e-04 - val_loss: 1.8137e-04\n",
      "Epoch 286/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0944e-04 - val_loss: 1.7058e-04\n",
      "Epoch 287/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0497e-04 - val_loss: 2.1372e-04\n",
      "Epoch 288/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0064e-04 - val_loss: 1.9953e-04\n",
      "Epoch 289/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 3.1006e-04 - val_loss: 1.9030e-04\n",
      "Epoch 290/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9457e-04 - val_loss: 2.0342e-04\n",
      "Epoch 291/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0254e-04 - val_loss: 1.7923e-04\n",
      "Epoch 292/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.9974e-04 - val_loss: 2.0012e-04\n",
      "Epoch 293/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 4.2539e-04 - val_loss: 2.1203e-04\n",
      "Epoch 294/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.2233e-04 - val_loss: 1.6502e-04\n",
      "Epoch 295/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0954e-04 - val_loss: 1.7431e-04\n",
      "Epoch 296/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9506e-04 - val_loss: 2.3715e-04\n",
      "Epoch 297/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0124e-04 - val_loss: 1.9977e-04\n",
      "Epoch 298/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.1245e-04 - val_loss: 1.8961e-04\n",
      "Epoch 299/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.1758e-04 - val_loss: 2.8313e-04\n",
      "Epoch 300/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0723e-04 - val_loss: 2.2175e-04\n",
      "Epoch 301/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.8555e-04 - val_loss: 2.3989e-04\n",
      "Epoch 302/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0998e-04 - val_loss: 1.6740e-04\n",
      "Epoch 303/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9949e-04 - val_loss: 2.0985e-04\n",
      "Epoch 304/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 4.3842e-04 - val_loss: 1.9795e-04\n",
      "Epoch 305/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.7129e-04 - val_loss: 2.0843e-04\n",
      "Epoch 306/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.1581e-04 - val_loss: 1.6733e-04\n",
      "Epoch 307/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9732e-04 - val_loss: 2.0096e-04\n",
      "Epoch 308/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1014e-04 - val_loss: 1.5232e-04\n",
      "Epoch 309/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.1204e-04 - val_loss: 2.4057e-04\n",
      "Epoch 310/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.9844e-04 - val_loss: 1.6568e-04\n",
      "Epoch 311/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.8300e-04 - val_loss: 1.8667e-04\n",
      "Epoch 312/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.9779e-04 - val_loss: 1.6141e-04\n",
      "Epoch 313/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0273e-04 - val_loss: 2.0550e-04\n",
      "Epoch 314/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9752e-04 - val_loss: 1.7020e-04\n",
      "Epoch 315/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9903e-04 - val_loss: 1.7626e-04\n",
      "Epoch 316/500\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 2.9763e-04 - val_loss: 1.6521e-04\n",
      "Epoch 317/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9777e-04 - val_loss: 2.0107e-04\n",
      "Epoch 318/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.2890e-04 - val_loss: 2.9964e-04\n",
      "Epoch 319/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.0622e-04 - val_loss: 2.1343e-04\n",
      "Epoch 320/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9743e-04 - val_loss: 1.9225e-04\n",
      "Epoch 321/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0154e-04 - val_loss: 1.8492e-04\n",
      "Epoch 322/500\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.7631e-04 - val_loss: 1.6796e-04\n",
      "Epoch 323/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.8517e-04 - val_loss: 1.7294e-04\n",
      "Epoch 324/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 3.9127e-04 - val_loss: 3.2719e-04\n",
      "Epoch 325/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.6777e-04 - val_loss: 1.7078e-04\n",
      "Epoch 326/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0624e-04 - val_loss: 1.7114e-04\n",
      "Epoch 327/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.0156e-04 - val_loss: 1.8995e-04\n",
      "Epoch 328/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9705e-04 - val_loss: 1.7677e-04\n",
      "Epoch 329/500\n",
      "162/162 [==============================] - 3s 19ms/step - loss: 2.9276e-04 - val_loss: 1.9258e-04\n",
      "Epoch 330/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.3348e-04 - val_loss: 1.8578e-04\n",
      "Epoch 331/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9709e-04 - val_loss: 1.7088e-04\n",
      "Epoch 332/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 3.3350e-04 - val_loss: 1.7984e-04\n",
      "Epoch 333/500\n",
      "162/162 [==============================] - 3s 20ms/step - loss: 2.9226e-04 - val_loss: 1.7566e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDrlL2pibPMw",
    "outputId": "fcf6530e-c85a-4dcf-d70c-efb58887bec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "model.save('models\\BPCL_model.h5')\n",
    "\n",
    "print(f\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4TU3TWu8PM2",
    "outputId": "1e04ecdf-f159-4e79-a595-ba69b17edd6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 10ms/step\n",
      "R-squared (R): 0.9974736343647608\n",
      "Mean Squared Error (MSE): 18.97243859006197\n",
      "Mean Absolute Error (MAE): 3.1950845576429203\n",
      "Mean Absolute Percentage Error (MAPE): 0.08417434335709627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,mean_absolute_percentage_error\n",
    "\n",
    "p_test = model.predict(X_test)\n",
    "p_test = scaler.inverse_transform(p_test)\n",
    "actual_prices = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "r2 = r2_score(actual_prices, p_test)\n",
    "mse = mean_squared_error(actual_prices, p_test)\n",
    "mae = mean_absolute_error(actual_prices, p_test)\n",
    "mape = mean_absolute_percentage_error(actual_prices, p_test)\n",
    "\n",
    "print(f'R-squared (R): {r2}')\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
